{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3e7773d",
   "metadata": {},
   "source": [
    "### CS 6840 Intro Machine Learning - Lab Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057fb92a",
   "metadata": {},
   "source": [
    "# <center>Building and Analyzing Classification Models</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd0a0d2",
   "metadata": {},
   "source": [
    "### 1. Overview\n",
    "The learning objective of this lab assignment is for students to understand different classification models, including how to train logistic regression, k-nearest neighbors, support vector machine, and decision tree with the impacts of key parameters, how to evaluate their classification performances, and how to compare these results among different classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633487e0",
   "metadata": {},
   "source": [
    "#### Lecture notes. \n",
    "Detailed coverage of these topics can be found in the following:\n",
    "<li>Logistic Regression</li>\n",
    "<li>Evaluation Metrics for Classification</li>\n",
    "<li>Cross Validation</li>\n",
    "<li>k-Nearest Neighbors</li>\n",
    "<li>Support Vector Machine</li>\n",
    "<li>Decision Tree</li>\n",
    "\n",
    "#### Code demonstrations.\n",
    "<li>Code 2023-09-20-W-Logistic Regression.ipynb</li>\n",
    "<li>Code 2023-09-25-M-Evaluation Metrics for Classification.ipynb</li>\n",
    "<li>Code 2023-09-27-W-Cross Validation.ipynb</li>\n",
    "<li>Code 2023-10-04-W-k-Nearest Neighbors.ipynb</li>\n",
    "<li>Code 2023-10-11-W-Soft Margin Classification SVM Model.ipynb</li>\n",
    "<li>Code 2023-10-16-M-Multi-class Classification and Kernel Trick of SVM.ipynb</li>\n",
    "<li>Code 2023-10-23-M-Decision Tree.ipynb</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d253216",
   "metadata": {},
   "source": [
    "### 2. Submission\n",
    "You need to submit a detailed lab report with code, running results, and answers to the questions. If you submit <font color='red'>a jupyter notebook (“Firstname-Lastname-6840-Lab2.ipynd”)</font>, please fill in this file directly and place the code, running results, and answers in order for each question. If you submit <font color='red'>a PDF report (“Firstname-Lastname-6840-Lab2.pdf”) with code file (“Firstname-Lastname-6840-Lab2.py”)</font>, please include the screenshots (code and running results) with answers for each question in the report.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7802ac85",
   "metadata": {},
   "source": [
    "### 3. Questions (50 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bffccc",
   "metadata": {},
   "source": [
    "For this lab assignment, you will be using the `housing dataset` to complete the following tasks and answer the questions. The housing dataset is the California Housing Prices dataset based on data from the 1990 California census. You will use these features to build classification models to predict the `ocean proximity` of a house. First, please place `housing.csv` and your notebook/python file in the same directory, and load and preprocess the data.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4353ca6",
   "metadata": {},
   "source": [
    "#### Load and preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98706103",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T12:12:15.297270Z",
     "start_time": "2023-11-10T12:12:13.591883Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Please place housing.csv and your notebook/python file in the same directory; otherwise, change DATA_PATH \n",
    "DATA_PATH = \"\"\n",
    "\n",
    "def load_housing_data(housing_path=DATA_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "housing = load_housing_data()\n",
    "\n",
    "#Add three useful features\n",
    "housing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\n",
    "housing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\n",
    "housing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]\n",
    "\n",
    "#Divide the data frame into features and labels\n",
    "housing_labels = housing[\"ocean_proximity\"].copy() # use ocean_proximity as classification label\n",
    "housing_features = housing.drop(\"ocean_proximity\", axis=1) # use colums other than ocean_proximity as features\n",
    "\n",
    "#Preprocessing the missing feature values\n",
    "median = housing_features[\"total_bedrooms\"].median()\n",
    "housing_features[\"total_bedrooms\"].fillna(median, inplace=True) \n",
    "median = housing_features[\"bedrooms_per_room\"].median()\n",
    "housing_features[\"bedrooms_per_room\"].fillna(median, inplace=True)\n",
    "\n",
    "#Scale the features\n",
    "std_scaler  = StandardScaler()\n",
    "housing_features_scaled = std_scaler.fit_transform(housing_features)\n",
    "\n",
    "#Final housing features X\n",
    "X = housing_features_scaled\n",
    "\n",
    "#Binary labels - 0: INLAND; 1: CLOSE TO OCEAN\n",
    "y_binary = (housing_labels != 1).astype(np.float64)\n",
    "#Multi-class labels - 0: <1H OCEAN; 1: INLAND; 2: NEAR OCEAN; 3: NEAR BAY\n",
    "y_multi = housing_labels.astype(np.float64)\n",
    "\n",
    "#Data splits for binary classification\n",
    "X_train_bi, X_test_bi, y_train_bi, y_test_bi = train_test_split(X, y_binary, test_size=0.20, random_state=42)\n",
    "\n",
    "#Data splits for multi-class classification\n",
    "X_train_mu, X_test_mu, y_train_mu, y_test_mu = train_test_split(X, y_multi, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e92572",
   "metadata": {},
   "source": [
    "<font color='red'><b>About the data used in this assignment: </b></font><br>\n",
    "**All the binary classification models are trained on `X_train_bi`, `y_train_bi`, and evaluated on `X_test_bi`, `y_test_bi`.**<br>\n",
    "**All the multi-class classification models are trained on `X_train_mu`, `y_train_mu`, and evaluated on `X_test_mu`, `y_test_mu`.**<br>\n",
    "**k-fold cross validation is performed directly on `X` and `y_multi`.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0ab7ee",
   "metadata": {},
   "source": [
    "#### Question 1 (4 points):  \n",
    "Please use features `X_train_bi` and binary labels `y_train_bi` to train a logistic regression binary classification model in function `answer_one( )`. After the model is trained, use `X_test_bi` and `y_test_bi` to evaluate the performance, including accuracy and F1 score.\n",
    "\n",
    "**Set `solver=\"newton-cg\"` and `random_state=42` in `LogisticRegression` to guarantee the convergence of train loss minimization** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac5a7ee2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T12:23:38.861641Z",
     "start_time": "2023-11-10T12:23:38.651182Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.9600290697674418\n",
      "F1 Score: 0.9710068529256721\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def answer_one():\n",
    "    #Train a binary_reg\n",
    "    binary_reg = LogisticRegression(solver=\"newton-cg\", random_state=42)\n",
    "    \n",
    "    #Use binary_reg to make prediction y_pred_bi\n",
    "    binary_reg.fit(X_train_bi, y_train_bi)\n",
    "    y_pred_bi = binary_reg.predict(X_test_bi)\n",
    "    \n",
    "    #Accuracy\n",
    "    binary_reg_accuracy = accuracy_score(y_test_bi, y_pred_bi)\n",
    "    \n",
    "    #F1 score\n",
    "    binary_reg_f1 = f1_score(y_test_bi, y_pred_bi)\n",
    "    \n",
    "    return binary_reg_accuracy, binary_reg_f1\n",
    "\n",
    "#Run your function in the cell to return the results\n",
    "accuracy_1, f1_1 = answer_one()\n",
    "print(\"Accuracy Score:\", accuracy_1)\n",
    "print(\"F1 Score:\", f1_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27d7e96",
   "metadata": {},
   "source": [
    "#### Answer 1:  \n",
    "Accuracy is: 96.00290697674418% <br>\n",
    "F1 score is: 97.10068529256721%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28237ee",
   "metadata": {},
   "source": [
    "#### Question 2 (4 points):  \n",
    "Please use features `X_train_mu` and multi-class labels `y_train_mu` to train a softmax regression multi-class classification model in function `answer_two( )`. After the model is trained, use `X_test_mu` and `y_test_mu` to evaluate the performance, including accuracy, micro F1 score, and macro F1 score.\n",
    "\n",
    "**Set `multi_class=\"multinomial\"`, `solver=\"newton-cg\"` and `random_state=42` in `LogisticRegression` to guarantee the convergence of multi-class training**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2312b55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T12:36:25.718063Z",
     "start_time": "2023-11-10T12:36:24.755608Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.7974806201550387\n",
      "Micro F1 Score:  0.7974806201550388\n",
      "Macro F1 Score:  0.6847642281014538\n"
     ]
    }
   ],
   "source": [
    "def answer_two():\n",
    "    #Train a multi_reg\n",
    "    multi_reg = LogisticRegression(multi_class=\"multinomial\", solver=\"newton-cg\", random_state=42)\n",
    "\n",
    "    #Use multi_reg to make prediction y_pred_mu\n",
    "    multi_reg.fit(X_train_mu, y_train_mu)\n",
    "    y_pred_mu = multi_reg.predict(X_test_mu)\n",
    "    \n",
    "    #Accuracy\n",
    "    multi_reg_accuracy = accuracy_score(y_test_mu, y_pred_mu)\n",
    "    \n",
    "    #Micro F1 score\n",
    "    multi_reg_microf1 = f1_score(y_test_mu, y_pred_mu, average='micro')\n",
    "    \n",
    "    #Macro F1 score\n",
    "    multi_reg_macrof1 = f1_score(y_test_mu, y_pred_mu, average='macro')\n",
    "    \n",
    "    return multi_reg_accuracy, multi_reg_microf1, multi_reg_macrof1\n",
    "\n",
    "#Run your function in the cell to return the results\n",
    "accuracy_2, microf1_2, macrof1_2 = answer_two()\n",
    "print(\"Accuracy Score: \", accuracy_2)\n",
    "print(\"Micro F1 Score: \", microf1_2)\n",
    "print(\"Macro F1 Score: \", macrof1_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4750a8ae",
   "metadata": {},
   "source": [
    "#### Answer 2:  \n",
    "Accuracy is: 79.74806201550387% <br>\n",
    "Micro f1 score is: 79.74806201550388% <br>\n",
    "Macro f1 score is: 68.47642281014538%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9290d7",
   "metadata": {},
   "source": [
    "#### Question 3 (6 points):  \n",
    "Please use features `X_train_bi` and binary labels `y_train_bi` to train a k-nearest neighbors binary classification model in function `answer_three( )`. After the model is trained, use `X_test_bi` and `y_test_bi` to evaluate the performance, including accuracy and F1 score.\n",
    "\n",
    "**Set the option `n_neighbors=` in `KNeighborsClassifier` using `1`, `3`, `5`, `7`, and `9` respectively to find an optimal value `k`**   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "745c3f80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T12:56:40.033266Z",
     "start_time": "2023-11-10T12:56:38.936220Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For k =  1  accuracy is:  0.9256298449612403  F1 Score is:  0.945615589016829\n",
      "For k =  3  accuracy is:  0.9367732558139535  F1 Score is:  0.9543307086614174\n",
      "For k =  5  accuracy is:  0.935077519379845  F1 Score is:  0.9533101045296168\n",
      "For k =  7  accuracy is:  0.9367732558139535  F1 Score is:  0.9546165884194053\n",
      "For k =  9  accuracy is:  0.9358042635658915  F1 Score is:  0.953953084274544\n",
      "Optimal k value:  7\n",
      "Optimal k accuracy:  0.9367732558139535\n",
      "Optimal k F1 Score:  0.9546165884194053\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def answer_three():\n",
    "    # Going to add a list of values to loop through to determine the optimal k value\n",
    "    potential_k_values = [1, 3, 5, 7, 9]\n",
    "    optimal_k = -1\n",
    "    optimal_k_accuracy = 0\n",
    "    optimal_k_f1 = 0\n",
    "    \n",
    "    # Loop through the potential k values training the model each time\n",
    "    for k in potential_k_values:\n",
    "        #Train a binary_knn\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        knn.fit(X_train_bi, y_train_bi)\n",
    "    \n",
    "        #Use binary_knn to make prediction y_pred_bi\n",
    "        y_pred_bi = knn.predict(X_test_bi)\n",
    "        \n",
    "        # Calculate potential k's accuracy and F1 score\n",
    "        potential_k_accuracy = accuracy_score(y_test_bi, y_pred_bi)\n",
    "        potential_k_f1 = f1_score(y_test_bi, y_pred_bi)\n",
    "        print(\"For k = \", k, \" accuracy is: \", potential_k_accuracy, \" F1 Score is: \", potential_k_f1)\n",
    "        \n",
    "        # Update the final accuracy and F1 if the scores are higher\n",
    "        if potential_k_accuracy > optimal_k_accuracy or (potential_k_accuracy == optimal_k_accuracy and potential_k_f1 > optimal_k_f1):\n",
    "            optimal_k = k\n",
    "            optimal_k_accuracy = potential_k_accuracy\n",
    "            optimal_k_f1 = potential_k_f1\n",
    "    \n",
    "    # Best k Score:\n",
    "    binary_k = optimal_k\n",
    "    \n",
    "    #Accuracy\n",
    "    binary_knn_accuracy = optimal_k_accuracy \n",
    "    \n",
    "    #F1 score\n",
    "    binary_knn_f1 = optimal_k_f1\n",
    "    \n",
    "    return binary_k, binary_knn_accuracy, binary_knn_f1\n",
    "\n",
    "#Run your function in the cell to return the results\n",
    "k_3, accuracy_3, f1_3 = answer_three()\n",
    "print(\"Optimal k value: \", k_3)\n",
    "print(\"Optimal k accuracy: \", accuracy_3)\n",
    "print(\"Optimal k F1 Score: \", f1_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6191991b",
   "metadata": {},
   "source": [
    "#### Answer 3:  \n",
    "When k = 1, accuracy is: 92.56298449612403%<br>\n",
    "When k = 3, accuracy is: 93.67732558139535%<br>\n",
    "When k = 5, accuracy is: 93.5077519379845%<br>\n",
    "When k = 7, accuracy is: 93.67732558139535%<br>\n",
    "When k = 9, accuracy is: 93.58042635658915%<br>\n",
    "Optimal k (`n_neighbors`) is: (7), accuracy is: 93.67732558139535%, F1 score is: 95.46165884194053%<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a769f5",
   "metadata": {},
   "source": [
    "#### Question 4 (7 points):  \n",
    "Please use features `X_train_mu` and multi-class labels `y_train_mu` to train a k-nearest neighbors multi-class classification model in function `answer_four( )`. After the model is trained, use `X_test_mu` and `y_test_mu` to evaluate the performance, including accuracy, micro F1 score, macro F1 score, loading time, and prediction time.\n",
    "\n",
    "**Set `n_neighbors=5` in `KNeighborsClassifier` and set the option `algorithm=` using `'brute'`, `'kd_tree'`, and `ball_tree` respectively to compare the different time used**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19561043",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T14:45:27.307959Z",
     "start_time": "2023-11-10T14:45:26.672684Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm: brute, Load Time: 0.002195119857788086s, Prediction Time: 0.04861092567443848s, Accuracy: 0.811046511627907, Micro F1: 0.811046511627907, Macro F1: 0.7542631097247448\n",
      "Algorithm: kd_tree, Load Time: 0.004649162292480469s, Prediction Time: 0.19477486610412598s, Accuracy: 0.811046511627907, Micro F1: 0.811046511627907, Macro F1: 0.7542631097247448\n",
      "Algorithm: ball_tree, Load Time: 0.004762887954711914s, Prediction Time: 0.3692941665649414s, Accuracy: 0.811046511627907, Micro F1: 0.811046511627907, Macro F1: 0.7542631097247448\n",
      "Best Load Time:\n",
      "brute  -  0.002195119857788086 seconds\n",
      "Best Prediction Time:\n",
      "brute  -  0.04861092567443848 seconds\n",
      "Best Accuracy:\n",
      "brute  -  0.811046511627907\n",
      "kd_tree  -  0.811046511627907\n",
      "ball_tree  -  0.811046511627907\n",
      "Best Micro F1:\n",
      "brute  -  0.811046511627907\n",
      "kd_tree  -  0.811046511627907\n",
      "ball_tree  -  0.811046511627907\n",
      "Best Macro F1:\n",
      "brute  -  0.7542631097247448\n",
      "kd_tree  -  0.7542631097247448\n",
      "ball_tree  -  0.7542631097247448\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def answer_four():\n",
    "    algorithms = ['brute', 'kd_tree', 'ball_tree']\n",
    "    results_a4 = []\n",
    "    fastest_load_time = {}\n",
    "    fastest_prediction_time = {}\n",
    "    highest_accuracy = {}\n",
    "    highest_micro_f1 = {}\n",
    "    highest_macro_f1 = {}\n",
    "    \n",
    "    for algorithm in algorithms:\n",
    "        #Add a time checkpoint here\n",
    "        load_time_start = time.time()\n",
    "        \n",
    "        #Train a multi_knn\n",
    "        multi_knn = KNeighborsClassifier(n_neighbors=5, algorithm=algorithm)\n",
    "        multi_knn.fit(X_train_mu, y_train_mu)\n",
    "        \n",
    "        #Add a time checkpoint here\n",
    "        load_time_end = time.time()\n",
    "        \n",
    "        #Use multi_knn to make prediction y_pred_mu\n",
    "        y_pred_mu = multi_knn.predict(X_test_mu)\n",
    "        \n",
    "        #Add a time checkpoint here\n",
    "        prediction_time_end = time.time()\n",
    "        \n",
    "        #Accuracy:\n",
    "        multi_knn_accuracy = accuracy_score(y_test_mu, y_pred_mu)\n",
    "        \n",
    "        #Micro F1 Score:\n",
    "        multi_knn_microf1 = f1_score(y_test_mu, y_pred_mu, average='micro')\n",
    "        \n",
    "        #Macro F1 Score:\n",
    "        multi_knn_macrof1 = f1_score(y_test_mu, y_pred_mu, average='macro')\n",
    "        \n",
    "        #time used for data loading:\n",
    "        multi_knn_loadtime = load_time_end - load_time_start\n",
    "        \n",
    "        #time used for prediction:\n",
    "        multi_knn_predictiontime = prediction_time_end - load_time_end\n",
    "        \n",
    "        # Add results to list of results\n",
    "        results_a4.append((algorithm, multi_knn_accuracy, multi_knn_microf1, multi_knn_macrof1, multi_knn_loadtime, multi_knn_predictiontime))\n",
    "\n",
    "        # Update the fastest load time\n",
    "        if not fastest_load_time or multi_knn_loadtime < next(iter(fastest_load_time.values())):\n",
    "            fastest_load_time = {algorithm: multi_knn_loadtime}\n",
    "        elif multi_knn_loadtime == next(iter(fastest_load_time.values())):\n",
    "            fastest_load_time[algorithm] = multi_knn_loadtime\n",
    "\n",
    "        # Update the fastest prediction time\n",
    "        if not fastest_prediction_time or multi_knn_predictiontime < next(iter(fastest_prediction_time.values())):\n",
    "            fastest_prediction_time = {algorithm: multi_knn_predictiontime}\n",
    "        elif multi_knn_predictiontime == next(iter(fastest_prediction_time.values())):\n",
    "            fastest_prediction_time[algorithm] = multi_knn_predictiontime\n",
    "\n",
    "        # Update the highest accuracy\n",
    "        if not highest_accuracy or multi_knn_accuracy > next(iter(highest_accuracy.values())):\n",
    "            highest_accuracy = {algorithm: multi_knn_accuracy}\n",
    "        elif multi_knn_accuracy == next(iter(highest_accuracy.values())):\n",
    "            highest_accuracy[algorithm] = multi_knn_accuracy\n",
    "\n",
    "        # Update the highest micro F1\n",
    "        if not highest_micro_f1 or multi_knn_microf1 > next(iter(highest_micro_f1.values())):\n",
    "            highest_micro_f1 = {algorithm: multi_knn_microf1}\n",
    "        elif multi_knn_microf1 == next(iter(highest_micro_f1.values())):\n",
    "            highest_micro_f1[algorithm] = multi_knn_microf1\n",
    "\n",
    "        # Update the highest macro F1\n",
    "        if not highest_macro_f1 or multi_knn_macrof1 > next(iter(highest_macro_f1.values())):\n",
    "            highest_macro_f1 = {algorithm: multi_knn_macrof1}\n",
    "        elif multi_knn_macrof1 == next(iter(highest_macro_f1.values())):\n",
    "            highest_macro_f1[algorithm] = multi_knn_macrof1\n",
    "    \n",
    "    return results_a4, fastest_load_time, fastest_prediction_time, highest_accuracy, highest_micro_f1, highest_macro_f1\n",
    "\n",
    "\n",
    "#Run your function in the cell to return the results\n",
    "results_4, load_time_4, prediction_time_4, accuracy_4, microf1_4, macrof1_4 = answer_four()\n",
    "for result in results_4:\n",
    "    print(f\"Algorithm: {result[0]}, Load Time: {result[4]}s, Prediction Time: {result[5]}s, Accuracy: {result[1]}, Micro F1: {result[2]}, Macro F1: {result[3]}\")\n",
    "\n",
    "print(\"Best Load Time:\")\n",
    "for key in load_time_4:\n",
    "    print(key, \" - \", load_time_4.get(key), \"seconds\")\n",
    "\n",
    "print(\"Best Prediction Time:\")\n",
    "for key in prediction_time_4:\n",
    "    print(key, \" - \", prediction_time_4.get(key), \"seconds\")\n",
    "    \n",
    "print(\"Best Accuracy:\")\n",
    "for key in accuracy_4:\n",
    "    print(key, \" - \", accuracy_4.get(key))\n",
    "\n",
    "print(\"Best Micro F1:\")\n",
    "for key in microf1_4:\n",
    "    print(key, \" - \", microf1_4.get(key))\n",
    "\n",
    "print(\"Best Macro F1:\")\n",
    "for key in macrof1_4:\n",
    "    print(key, \" - \", macrof1_4.get(key))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859f25dc",
   "metadata": {},
   "source": [
    "#### Answer 4:  \n",
    "<b>Brute force: </b> data loading time is: 0.002195119857788086s, prediction time is: 0.04861092567443848s, accuracy is: 81.1046511627907%, micro f1 score is: 81.1046511627907%, macro f1 score is: 75.42631097247448% <br>\n",
    "<b>K-d tree: </b> data loading time is: 0.004649162292480469s, prediction time is: 0.19477486610412598s, accuracy is: 81.1046511627907%, micro f1 score is: 81.1046511627907%, macro f1 score is: 75.42631097247448% <br>\n",
    "<b>Ball tree: </b> data loading time is: 0.004762887954711914s, prediction time is: 0.3692941665649414s, accuracy is: 81.1046511627907%, micro f1 score is: 81.1046511627907%, macro f1 score is: 75.42631097247448% <br>\n",
    "Summarize your observations about the time used by these searching algorithms: The Brute force algorithm had the fastest loading time and the fastest prediction time and observations about the classification performance: Overall, all three algorithms have the same scores for accuracy, micro F1 and macro F1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c978095",
   "metadata": {},
   "source": [
    "#### Question 5 (7 points):  \n",
    "Please use features `X_train_bi` and binary labels `y_train_bi` to train a support vector machine binary classification model in function `answer_five( )`. After the model is trained, use `X_test_bi` and `y_test_bi` to evaluate the performance, including accuracy and F1 score.\n",
    "\n",
    "**Set `random_state=42` in `SVC`, and set the kernel function `kernel=` using `'linear'`, `'rbf'`, and `'poly'` respectively to compare different performance** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf2bf240",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T15:57:50.689388Z",
     "start_time": "2023-11-10T15:57:46.683691Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: linear, Accuracy: 0.9617248062015504, F1 Score: 0.9721340388007055\n",
      "\n",
      "Kernel: rbf, Accuracy: 0.9656007751937985, F1 Score: 0.9751574527641709\n",
      "\n",
      "Kernel: poly, Accuracy: 0.9341085271317829, F1 Score: 0.9532967032967032\n",
      "\n",
      "Best Performing Model:\n",
      "\n",
      "Kernel: rbf, Accuracy: 0.9656007751937985, F1 Score: 0.9751574527641709\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "def answer_five():\n",
    "    # Create a list of models and a list to store the results\n",
    "    kernels = ['linear', 'rbf', 'poly']\n",
    "    results_a5 = []\n",
    "    \n",
    "    # Create a dictionary to store the best performing model\n",
    "    best_perfomance = {'Kernel': '', 'Accuracy': 0, 'F1': 0}\n",
    "    \n",
    "    # Traverse the list of models\n",
    "    for kernel in kernels:\n",
    "        \n",
    "        #Train a binary_svm\n",
    "        binary_svm = SVC(kernel=kernel, random_state=42)\n",
    "        binary_svm.fit(X_train_bi, y_train_bi)\n",
    "\n",
    "        #Use binary_svm to make prediction y_pred_bi\n",
    "        y_pred_bi = binary_svm.predict(X_test_bi)\n",
    "    \n",
    "        #Accuracy\n",
    "        binary_svm_accuracy = accuracy_score(y_test_bi, y_pred_bi)\n",
    "        \n",
    "        #F1 score\n",
    "        binary_svm_f1 = f1_score(y_test_bi, y_pred_bi)\n",
    "        \n",
    "        # Add result to list of results\n",
    "        results_a5.append((kernel, binary_svm_accuracy, binary_svm_f1))\n",
    "        \n",
    "        # Update the best performing model\n",
    "        if binary_svm_accuracy > best_perfomance['Accuracy'] or (binary_svm_accuracy == best_perfomance['Accuracy'] and binary_svm_f1 > best_perfomance['F1']):\n",
    "            best_perfomance['Kernel'] = kernel\n",
    "            best_perfomance['Accuracy'] = binary_svm_accuracy\n",
    "            best_perfomance['F1'] = binary_svm_f1\n",
    "    \n",
    "    return results_a5, best_perfomance\n",
    "\n",
    "#Run your function in the cell to return the results\n",
    "results_5, best_performance_5 = answer_five()\n",
    "for result in results_5:\n",
    "    print(f\"Kernel: {result[0]}, Accuracy: {result[1]}, F1 Score: {result[2]}\\n\")\n",
    "\n",
    "print(\"Best Performing Model:\\n\")\n",
    "print(f\"Kernel: {best_performance_5['Kernel']}, Accuracy: {best_performance_5['Accuracy']}, F1 Score: {best_performance_5['F1']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2c2a70",
   "metadata": {},
   "source": [
    "#### Answer 5:  \n",
    "<b>Linear kernel: </b> accuracy is: 96.17248062015504%, and f1 score is: 97.21340388007055% <br> \n",
    "<b>RBF kernel: </b> accuracy is: 96.56007751937985%, and f1 score is: 97.51574527641709% <br> \n",
    "<b>Polynomial kernel: </b> accuracy is: 93.41085271317829%, and f1 score is: 95.32967032967032% <br>\n",
    "Summarize your observations about the performance derived by these different kernels: The best performing kernel was the RBF kernel which had both the highest accuracy and F1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74087a8",
   "metadata": {},
   "source": [
    "#### Question 6 (6 points):\n",
    "Please use features `X_train_mu` and multi-class labels `y_train_mu` to train a support vector machine multi-class classification model in function `answer_six( )`. After the model is trained, use `X_test_mu` and `y_test_mu` to evaluate the performance, including accuracy, micro F1 score, and macro F1 score.\n",
    "\n",
    "**Set `kernel='rbf'`, `random_state=42` in `SVC`, and set `decision_function_shape=` using `'ovr'` and `'ovo'` respectively to compare different performance and time cost**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d38e6ec1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T16:30:38.523568Z",
     "start_time": "2023-11-10T16:30:33.582942Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Function: ovr, Training Time: 2.510913133621216s, Accuracy: 0.8439922480620154, Micro F1: 0.8439922480620154, Macro F1: 0.7813377183100643\n",
      "Decision Function: ovo, Training Time: 2.421372890472412s, Accuracy: 0.8439922480620154, Micro F1: 0.8439922480620154, Macro F1: 0.7813377183100643\n"
     ]
    }
   ],
   "source": [
    "def answer_six():\n",
    "    \n",
    "    # Create a list of decision functions and an empty list to store results\n",
    "    decision_functions = ['ovr', 'ovo']\n",
    "    results_a6 = []\n",
    "    \n",
    "    \n",
    "    # Traverse the decision_function list and train the SVM\n",
    "    for decision_function in decision_functions:\n",
    "        \n",
    "        #Add a time checkpoint here\n",
    "        time1 = time.time()\n",
    "        \n",
    "        #Train a multi_svm\n",
    "        multi_svm = SVC(kernel='rbf', decision_function_shape=decision_function, random_state=42)\n",
    "        multi_svm.fit(X_train_mu, y_train_mu)\n",
    "    \n",
    "        #Use multi_svm to make prediction y_pred_mu\n",
    "        y_pred_mu = multi_svm.predict(X_test_mu)\n",
    "        \n",
    "        #Add a time checkpoint here\n",
    "        time2 = time.time()\n",
    "        \n",
    "        #Accuracy\n",
    "        multi_svm_accuracy = accuracy_score(y_test_mu, y_pred_mu)\n",
    "        \n",
    "        #Micro F1 score\n",
    "        multi_svm_microf1 = f1_score(y_test_mu, y_pred_mu, average='micro')\n",
    "        \n",
    "        #Macro F1 score\n",
    "        multi_svm_macrof1 = f1_score(y_test_mu, y_pred_mu, average='macro')\n",
    "        \n",
    "        #time used\n",
    "        multi_svm_time = time2 - time1\n",
    "        \n",
    "        results_a6.append((decision_function, multi_svm_time, multi_svm_accuracy, multi_svm_microf1, multi_svm_macrof1))\n",
    "    \n",
    "    return results_a6\n",
    "\n",
    "#Run your function in the cell to return the results\n",
    "results_6 = answer_six()\n",
    "for result in results_6:\n",
    "    print(f\"Decision Function: {result[0]}, Training Time: {result[1]}s, Accuracy: {result[2]}, Micro F1: {result[3]}, Macro F1: {result[4]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b673a0",
   "metadata": {},
   "source": [
    "#### Answer 6:  \n",
    "<b>One-vs-one (ovo): </b> time used is: 2.421372890472412s, accuracy is: 84.39922480620154%, micro f1 score is: 84.39922480620154%, macro f1 score is: 78.13377183100643% <br>\n",
    "<b>One-vs-rest (ovr): </b> time used is: 2.510913133621216s, accuracy is: 84.39922480620154%, micro f1 score is: 84.39922480620154%, macro f1 score is: 78.13377183100643% <br>\n",
    "Summarize your observations about the time used by these multi-class methods: The one-vs-one decision function was faster than the one-vs-rest and observations about the classification performance: other than the training time, both models had the same scores for accuracy and F1's meaning that ovo was the better option since it achieved the same result in less time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d01ff7e",
   "metadata": {},
   "source": [
    "#### Question 7 (3 points):\n",
    "<font color='red'><b>Double click here to answer the questions in this cell: </b></font><br>\n",
    "Based on the results from Question 1 to Question 6: <br>\n",
    "The model with best binary classification performance is: SVM w/ RBF <br>\n",
    "The model with worst binary classification performance is: kNN <br>\n",
    "The model with best multi-class classification performance is: SVM w/ OVO <br>\n",
    "The model with worst multi-class classification performance is: Softmax Regression <br>\n",
    "Summarize your personal thoughts on the model choices: The binary models all outperform the multi-class models, but they will restrict the breadth of predictions that can be made because they are simply binary models. That said, while the multiclass SVM w/ OVO had the best performance for any of the multiclass models, it also had the worst performance in terms of time. When working with small datasets like this it isn't an issue, but would likely be a major factor when scaled up to much larger datasets. Overall, the tradeoff appears to be that more complex predictions tend to be less accurate than the simple binary predictions, as well as that increasing the accuracy of these complex predictions tends to negatively impact model performance in terms of time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38c006d",
   "metadata": {},
   "source": [
    "#### Question 8 (6 points):\n",
    "Please use `X_train_mu, X_test_mu, y_train_mu, y_test_mu = train_test_split(X, y_multi, test_size=0.40)` to perform data splits for 5 different times, and each time, use `X_train_mu` and `y_train_mu` to train a decision tree in function `answer_eight( )`. After the model is trained, use `X_test_mu` and `y_test_mu` to evaluate the performance, including accuracy, micro F1 score, and macro F1 score.\n",
    "\n",
    "**Set `max_depth=4`, `random_state=42` and `criterion='gini'` in `DecisionTreeClassifier`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "944d3b7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T18:27:50.575539Z",
     "start_time": "2023-11-10T18:27:50.333684Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run #:  1\n",
      "Accuracy:  0.8313953488372093\n",
      "Micro F1:  0.8313953488372093\n",
      "Macro F1:  0.766801220060083\n",
      "Run #:  2\n",
      "Accuracy:  0.843265503875969\n",
      "Micro F1:  0.843265503875969\n",
      "Macro F1:  0.7833267119512235\n",
      "Run #:  3\n",
      "Accuracy:  0.8270348837209303\n",
      "Micro F1:  0.8270348837209303\n",
      "Macro F1:  0.7781206898058822\n",
      "Run #:  4\n",
      "Accuracy:  0.8349079457364341\n",
      "Micro F1:  0.8349079457364341\n",
      "Macro F1:  0.7704587777706714\n",
      "Run #:  5\n",
      "Accuracy:  0.8399951550387597\n",
      "Micro F1:  0.8399951550387597\n",
      "Macro F1:  0.7702341098959629\n",
      "Average for All:\n",
      "Average Accuracy:  0.8353197674418604\n",
      "Average Micro F1 Score:  0.8353197674418604\n",
      "Average Macro F1 Score:  0.7737883018967646\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def answer_eight():\n",
    "    \n",
    "    # Create lists to store accuracy, micro f1, and macro f1 for each split\n",
    "    accuracies = []\n",
    "    micro_f1s = []\n",
    "    macro_f1s = []\n",
    "    \n",
    "    # Loop for 5 runs\n",
    "    for run in range(5):\n",
    "        X_train_mu, X_test_mu, y_train_mu, y_test_mu = train_test_split(X, y_multi, test_size=0.40)\n",
    "        \n",
    "        multi_dt = DecisionTreeClassifier(max_depth=4, criterion='gini', random_state=42)\n",
    "        multi_dt.fit(X_train_mu, y_train_mu)\n",
    "      \n",
    "        y_pred_mu = multi_dt.predict(X_test_mu)\n",
    "        \n",
    "        #Accuracy\n",
    "        multi_dt_accuracy = accuracy_score(y_test_mu, y_pred_mu)\n",
    "        accuracies.append(multi_dt_accuracy)\n",
    "        \n",
    "        #Micro F1 score\n",
    "        multi_dt_microf1 = f1_score(y_test_mu, y_pred_mu, average='micro')\n",
    "        micro_f1s.append(multi_dt_microf1)\n",
    "        \n",
    "        #Macro F1 score\n",
    "        multi_dt_macrof1 = f1_score(y_test_mu, y_pred_mu, average='macro')\n",
    "        macro_f1s.append(multi_dt_macrof1)\n",
    "        \n",
    "        # Print info for current split\n",
    "        print(\"Run #: \", run + 1)\n",
    "        print(\"Accuracy: \", multi_dt_accuracy)\n",
    "        print(\"Micro F1: \", multi_dt_microf1)\n",
    "        print(\"Macro F1: \", multi_dt_macrof1)\n",
    "        \n",
    "    avg_accuracy = sum(accuracies) / len(accuracies)\n",
    "    avg_microf1 = sum(micro_f1s) / len(micro_f1s)\n",
    "    avg_macrof1 = sum(macro_f1s) / len(macro_f1s)\n",
    "    \n",
    "    return avg_accuracy,avg_microf1, avg_macrof1 \n",
    "\n",
    "#Run your function in the cell to return the results\n",
    "accuracy_8, microf1_8, macrof1_8 = answer_eight()\n",
    "print(\"Average for All:\")\n",
    "print(\"Average Accuracy: \", accuracy_8)\n",
    "print(\"Average Micro F1 Score: \", microf1_8)\n",
    "print(\"Average Macro F1 Score: \", macrof1_8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1fb4e6",
   "metadata": {},
   "source": [
    "#### Answer 8:  \n",
    "First run: <br>\n",
    "Accuracy is: 83.13953488372093%, Micro f1 score is: 83.13953488372093%, Macro f1 score is: 76.6801220060083% <br><br>\n",
    "Second run: <br>\n",
    "Accuracy is: 84.3265503875969%, Micro f1 score is: 84.3265503875969%, Macro f1 score is: 78.33267119512235% <br><br>\n",
    "Third run: <br>\n",
    "Accuracy is: 82.70348837209303%, Micro f1 score is: 82.70348837209303%, Macro f1 score is: 77.81206898058822% <br><br>\n",
    "Fourth run: <br>\n",
    "Accuracy is: 83.49079457364341%, Micro f1 score is: 83.49079457364341%, Macro f1 score is: 77.04587777706714% <br><br>\n",
    "Fifth run: <br>\n",
    "Accuracy is: 83.99951550387597%, Micro f1 score is: 83.99951550387597%, Macro f1 score is: 77.02341098959629% <br><br>\n",
    "Summarize your observations why these results vary and the disadvantages of hold-out evaluation: The accuracy and micro F1 scores are the same for each run since they are mathematically equivalent with multiclass models. This value did vary between runs, but not by a lot (82.7% to 84.3% or a variance of roughly 1.6%). This indicates that the model performance is affected by the data split used for training and testing. The macro F1 score shows similar variability. These variations point out a key disadvantage of hold-out evaluation in that it is dependent on the data split. These different splits will create different distributions of classes in the training and test datasets, thus affecting the model performance. This could potentially be much worse if the datasets are much smaller or if they are even more imbalanced between the training and test datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d13dabd",
   "metadata": {},
   "source": [
    "#### Question 9 (7 points):\n",
    "Please use `X` and `y_multi` to implement k-fold cross validation in function `answer_nine( )` to evaluate decision tree multi-class classification model, including the mean of accuracy, micro F1 score, and macro F1 score.\n",
    "\n",
    "**Set `max_depth=4`, `random_state=42` and `criterion='gini'` in `DecisionTreeClassifier`**\n",
    "\n",
    "**Set `cv=5` and `scoring=(\"accuracy\", \"f1_micro\", \"f1_macro\")` in `cross_validate` to return the cross-validation evaluation results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7ac5e57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T19:13:26.262098Z",
     "start_time": "2023-11-10T19:13:25.978356Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.6973837209302326\n",
      "Mean Micro F1 Score: 0.6973837209302326\n",
      "Mean Macro F1 Score: 0.6475799195256458\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from statistics import mean\n",
    "\n",
    "def answer_nine():\n",
    "    multi_dt = DecisionTreeClassifier(max_depth=4, criterion='gini', random_state=42)\n",
    "    \n",
    "    #Cross validation evaluation\n",
    "    cv_results = cross_validate(multi_dt, X, y_multi, cv=5, scoring=(\"accuracy\", \"f1_micro\", \"f1_macro\"))\n",
    "    \n",
    "    #Accuracy: use mean()\n",
    "    multi_dt_accuracy = mean(cv_results['test_accuracy'])\n",
    "    \n",
    "    #Micro F1 score: use mean()\n",
    "    multi_dt_microf1 = mean(cv_results['test_f1_micro'])\n",
    "    \n",
    "    #Macro F1 score: use mean()\n",
    "    multi_dt_macrof1 = mean(cv_results['test_f1_macro'])\n",
    "    \n",
    "    return multi_dt_accuracy, multi_dt_microf1, multi_dt_macrof1\n",
    "\n",
    "#Run your function in the cell to return the results\n",
    "accuracy_9, microf1_9, macrof1_9 = answer_nine()\n",
    "print(\"Mean Accuracy:\", accuracy_9)\n",
    "print(\"Mean Micro F1 Score:\", microf1_9)\n",
    "print(\"Mean Macro F1 Score:\", macrof1_9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65439330",
   "metadata": {},
   "source": [
    "#### Answer 9:  \n",
    "Accuracy using 5-fold cross validation is: 69.73837209302326% <br>\n",
    "Micro f1 score using 5-fold cross validation is: 69.73837209302326% <br>\n",
    "Macro f1 score using 5-fold cross validation is: 64.75799195256458% <br>\n",
    "Compared to the classification results in Question 8, what is your observation: the accuracy and F1 scores are much lower than the results from question 8, and why that happens: this is likely because question 8 had a heavily biased estimate of the model's performance. If the hold-out test set was not representative of the overall dataset, it would explain having the much lower cross validation scores. <br>\n",
    "Summarize the advantages and disadvantages of cross validation: Overall cross validation offers a more reliable/complete assessment of how a model is performing compared to a single hold-out validation. This helps to reduce bias, better utilize the data, and to provide more accurate estimates since the model is evaluated on multiple subsets within the data. Unfortunately this will have a negative impact when using much larger datasets, presenting a scaling problem. The other challenge is that since cross validation involves training multiple models on subsets of the data, there is not a unified model that is validated (unless you retrain on the entire dataset, which would not be validated as a whole). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1c7d3c13ca085cf0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
